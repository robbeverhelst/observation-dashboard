apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: observation-explorer-alerts
  namespace: observation-explorer
  labels:
    app: observation-explorer
    release: prometheus
spec:
  groups:
    - name: observation-explorer.performance
      interval: 30s
      rules:
        - alert: HighResponseTime
          expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{service_name="observation-explorer"}[5m])) > 3
          for: 2m
          labels:
            severity: warning
            component: api
          annotations:
            summary: 'High API response time detected'
            description: '95th percentile response time is {{ $value }}s, above 3s threshold'

        - alert: CriticalResponseTime
          expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{service_name="observation-explorer"}[5m])) > 10
          for: 1m
          labels:
            severity: critical
            component: api
          annotations:
            summary: 'Critical API response time detected'
            description: '95th percentile response time is {{ $value }}s, above 10s threshold'

    - name: observation-explorer.cache
      interval: 30s
      rules:
        - alert: LowCacheHitRate
          expr: (sum(rate(cache_hits_total[5m])) / (sum(rate(cache_hits_total[5m])) + sum(rate(cache_misses_total[5m])))) * 100 < 70
          for: 5m
          labels:
            severity: warning
            component: cache
          annotations:
            summary: 'Low cache hit rate detected'
            description: 'Cache hit rate is {{ $value }}%, below 70% threshold'

        - alert: CriticalCacheHitRate
          expr: (sum(rate(cache_hits_total[5m])) / (sum(rate(cache_hits_total[5m])) + sum(rate(cache_misses_total[5m])))) * 100 < 50
          for: 2m
          labels:
            severity: critical
            component: cache
          annotations:
            summary: 'Critical cache hit rate detected'
            description: 'Cache hit rate is {{ $value }}%, below 50% threshold'

    - name: observation-explorer.errors
      interval: 30s
      rules:
        - alert: HighErrorRate
          expr: sum(rate(application_errors_total[5m])) > 5
          for: 2m
          labels:
            severity: warning
            component: application
          annotations:
            summary: 'High application error rate'
            description: 'Application generating {{ $value }} errors per second'

        - alert: ExternalAPIFailures
          expr: sum(rate(external_api_requests_total{service="waarneming_nl", status="error"}[5m])) / sum(rate(external_api_requests_total{service="waarneming_nl"}[5m])) * 100 > 10
          for: 5m
          labels:
            severity: warning
            component: external-api
          annotations:
            summary: 'High external API failure rate'
            description: '{{ $value }}% of external API requests are failing'

    - name: observation-explorer.resources
      interval: 30s
      rules:
        - alert: HighMemoryUsage
          expr: process_resident_memory_bytes{service_name="observation-explorer"} / 1024 / 1024 > 400
          for: 5m
          labels:
            severity: warning
            component: resources
          annotations:
            summary: 'High memory usage detected'
            description: 'Application using {{ $value }}MB of memory, above 400MB threshold'

        - alert: CriticalMemoryUsage
          expr: process_resident_memory_bytes{service_name="observation-explorer"} / 1024 / 1024 > 480
          for: 2m
          labels:
            severity: critical
            component: resources
          annotations:
            summary: 'Critical memory usage detected'
            description: 'Application using {{ $value }}MB of memory, approaching 512MB limit'

    - name: observation-explorer.availability
      interval: 30s
      rules:
        - alert: ServiceDown
          expr: up{job=~".*observation-explorer.*"} == 0
          for: 1m
          labels:
            severity: critical
            component: availability
          annotations:
            summary: 'Observation Explorer service is down'
            description: 'Service {{ $labels.instance }} has been down for more than 1 minute'

        - alert: NoNewDataDetections
          expr: increase(new_data_detections_total[30m]) == 0
          for: 30m
          labels:
            severity: warning
            component: data-freshness
          annotations:
            summary: 'No new data detections'
            description: 'No new observation data has been detected in the last 30 minutes'
